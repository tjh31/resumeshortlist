{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Using cached tika-2.6.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: setuptools in c:\\users\\harde\\anaconda3\\lib\\site-packages (from tika) (68.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\harde\\anaconda3\\lib\\site-packages (from tika) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from requests->tika) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from requests->tika) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from requests->tika) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from requests->tika) (2023.11.17)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32687 sha256=df92d638ee9f494ab37121868b1e168675bdf7545e5daa0541abadd03aa5b2e0\n",
      "  Stored in directory: c:\\users\\harde\\appdata\\local\\pip\\cache\\wheels\\27\\ba\\2f\\37420d1191bdae5e855d69b8e913673045bfd395cbd78ad697\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harde\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\harde\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\harde\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harde\\anaconda3\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\harde\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 20:06:32,876 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    }
   ],
   "source": [
    "#Extracting text from resume pdfs\n",
    "from tika import parser\n",
    "import os\n",
    "\n",
    "directory_path = \"path_to_folder_with_Resumes\"\n",
    "# Create an empty list to store the extracted text\n",
    "list_files = []\n",
    "for file in os.listdir(directory_path):\n",
    "    # Check if the file is a PDF file\n",
    "    if file.endswith('.pdf'):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        \n",
    "        # Use Tika to parse the PDF file\n",
    "        raw = parser.from_file(file_path)\n",
    "        \n",
    "        # Append the extracted content to the list\n",
    "        list_files.append(raw['content'])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Terrence Coleman\n",
      "tcoleman@email.com (123) 456-7890 Brooklyn, NY LinkedIn\n",
      "\n",
      "Analytically minded self-starter with a decade of experience collaborating with cross-functional teams and\n",
      "ensuring the accuracy and integrity around data and actionable insights. Prepared to lead teams and interns in\n",
      "\n",
      "predictive modeling and insight reporting to boost Hyphen's business efficiency, strategic goals, and profit.\n",
      "\n",
      "WORK EXPERIENCE\n",
      "Best Buy - Senior Data Scientist\n",
      "October 2018 - current Remote\n",
      "\n",
      "· Led data extraction and evaluation efforts to save Best Buy more than 11M over the course of tenure\n",
      "\n",
      "· Partnered with product team to build a production recommendation engine in Python that improved the\n",
      "average length on page for users and resulted in $450K in incremental annual revenue\n",
      "\n",
      "· Created a customer attrition random forest model, improving monthly retention by 6 basis points for\n",
      "customers likely to attrit by servicing relevant product features for them\n",
      "\n",
      "· Communicated with PMs to lead 4 data scientists in project planning, development, and execution\n",
      "\n",
      "· Coached data team throughout short and long-term projects, redefining documentation frequently\n",
      "\n",
      "2U - Data Scientist\n",
      "April 2014 - October 2018 Brooklyn, NY\n",
      "\n",
      "· Conducted A/B testing to solve client pain points in learning platforms, and identified and recommended\n",
      "solutions to solve unclear platform roadmaps, which reduced the bounce rate by 62%\n",
      "\n",
      "· Extracted data from 7 disparate sources, and increased agility and accuracy with a centralized system\n",
      "\n",
      "· Constructed decisions trees to optimize needed algorithms to better target the learning audience by 15%\n",
      "\n",
      "2U - Data Analyst\n",
      "April 2012 - April 2014 Brooklyn, NY\n",
      "\n",
      "· Determined, using Python clustering methods, groups of states where underwriting models were\n",
      "underperforming, and owned improvements to increase profit by 4%\n",
      "\n",
      "· Identified procedural areas of improvement through customer data to help improve the profitability of a\n",
      "nationwide retention program by 8%\n",
      "\n",
      "· Developed and owned the reporting for a nationwide retention program using Python, SQL, and Excel,\n",
      "saving an average of 60 hours of labor each month\n",
      "\n",
      "EDUCATION\n",
      "University of Pittsburgh - Master's , Mathematics\n",
      "September 2012 - April 2014 Pittsburgh, PA\n",
      "\n",
      "University of Pittsburgh - Bachelor's, Mathematics and Economics\n",
      "September 2008 - April 2012 Pittsburgh, PA\n",
      "\n",
      "SKILLS\n",
      "Python (NumPy, Pandas, Scikit-learn, Flask), SAS; SQL - Redshift, MySQL; ElasticSearch; Recommendation\n",
      "Engines, Customer Segmentation & Retention Models, Price Optimization, Productionizing Models\n",
      "\n",
      "http://linkedin.com/in/terrence-coleman\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list_files[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  \n",
    "preprocessed_files = []\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    punctuation_to_remove = string.punctuation.replace('@', '').replace('.', '')\n",
    "    \n",
    "    translation_table = str.maketrans('', '', punctuation_to_remove)\n",
    "    \n",
    "    text = text.translate(translation_table)\n",
    "    \n",
    "  \n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    \n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "for text in list_files:\n",
    "    # Preprocess the text\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    \n",
    "    # Append the preprocessed text to preprocessed_files list\n",
    "    preprocessed_files.append(preprocessed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrence coleman tcoleman@email.com 123 4567890 brooklyn ny linkedin analytically minded selfstarter decade experience collaborating crossfunctional team ensuring accuracy integrity around data actionable insights. prepared lead team intern predictive modeling insight reporting boost hyphen business efficiency strategic goal profit. work experience best buy senior data scientist october 2018 current remote · led data extraction evaluation effort save best buy 11m course tenure · partnered product team build production recommendation engine python improved average length page user resulted 450k incremental annual revenue · created customer attrition random forest model improving monthly retention 6 basis point customer likely attrit servicing relevant product feature · communicated pm lead 4 data scientist project planning development execution · coached data team throughout short longterm project redefining documentation frequently 2u data scientist april 2014 october 2018 brooklyn ny · conducted ab testing solve client pain point learning platform identified recommended solution solve unclear platform roadmaps reduced bounce rate 62 · extracted data 7 disparate source increased agility accuracy centralized system · constructed decision tree optimize needed algorithm better target learning audience 15 2u data analyst april 2012 april 2014 brooklyn ny · determined using python clustering method group state underwriting model underperforming owned improvement increase profit 4 · identified procedural area improvement customer data help improve profitability nationwide retention program 8 · developed owned reporting nationwide retention program using python sql excel saving average 60 hour labor month education university pittsburgh master mathematics september 2012 april 2014 pittsburgh pa university pittsburgh bachelor mathematics economics september 2008 april 2012 pittsburgh pa skill python numpy panda scikitlearn flask sa sql redshift mysql elasticsearch recommendation engine customer segmentation retention model price optimization productionizing model httplinkedin.cominterrencecoleman\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_files[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPdf2 in c:\\users\\harde\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in c:\\users\\harde\\anaconda3\\lib\\site-packages (0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fitz\n",
      "  Using cached fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
      "Collecting configobj (from fitz)\n",
      "  Using cached configobj-5.0.8-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting configparser (from fitz)\n",
      "  Using cached configparser-7.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting httplib2 (from fitz)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nibabel (from fitz)\n",
      "  Using cached nibabel-5.2.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting nipype (from fitz)\n",
      "  Using cached nipype-1.8.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\harde\\anaconda3\\lib\\site-packages (from fitz) (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\harde\\anaconda3\\lib\\site-packages (from fitz) (2.1.4)\n",
      "Collecting pyxnat (from fitz)\n",
      "  Using cached pyxnat-1.6.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\harde\\anaconda3\\lib\\site-packages (from fitz) (1.11.4)\n",
      "Requirement already satisfied: six in c:\\users\\harde\\anaconda3\\lib\\site-packages (from configobj->fitz) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from httplib2->fitz) (3.0.9)\n",
      "Requirement already satisfied: packaging>=17 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from nibabel->fitz) (23.2)\n",
      "Requirement already satisfied: click>=6.6.0 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from nipype->fitz) (8.1.7)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from nipype->fitz) (3.1)\n",
      "Collecting prov>=1.5.2 (from nipype->fitz)\n",
      "  Using cached prov-2.0.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting pydot>=1.2.3 (from nipype->fitz)\n",
      "  Using cached pydot-2.0.0-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from nipype->fitz) (2.8.2)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Using cached rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting simplejson>=3.8.0 (from nipype->fitz)\n",
      "  Using cached simplejson-3.19.2-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting traits!=5.0,<6.4,>=4.6 (from nipype->fitz)\n",
      "  Using cached traits-6.3.2.tar.gz (9.5 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: filelock>=3.0.0 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from nipype->fitz) (3.13.1)\n",
      "Collecting etelemetry>=0.2.0 (from nipype->fitz)\n",
      "  Using cached etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting looseversion (from nipype->fitz)\n",
      "  Using cached looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from pandas->fitz) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from pandas->fitz) (2023.4)\n",
      "Requirement already satisfied: lxml>=4.3 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from pyxnat->fitz) (5.1.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from pyxnat->fitz) (2.31.0)\n",
      "Requirement already satisfied: pathlib>=1.0 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\harde\\anaconda3\\lib\\site-packages (from click>=6.6.0->nipype->fitz) (0.4.6)\n",
      "Collecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype->fitz)\n",
      "  Using cached ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harde\\anaconda3\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (2023.11.17)\n",
      "Using cached fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Using cached configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
      "Using cached configparser-7.0.0-py3-none-any.whl (16 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached nibabel-5.2.1-py3-none-any.whl (3.3 MB)\n",
      "Using cached nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
      "Using cached pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
      "Using cached etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Using cached prov-2.0.0-py3-none-any.whl (421 kB)\n",
      "Using cached pydot-2.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
      "Using cached simplejson-3.19.2-cp311-cp311-win_amd64.whl (75 kB)\n",
      "Using cached looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Building wheels for collected packages: traits\n",
      "  Building wheel for traits (setup.py): started\n",
      "  Building wheel for traits (setup.py): finished with status 'done'\n",
      "  Created wheel for traits: filename=traits-6.3.2-cp311-cp311-win_amd64.whl size=5000848 sha256=ffa577ae1845b12c3215280193c4b37cd10b79974904f3b94d250819ca2fd5e1\n",
      "  Stored in directory: c:\\users\\harde\\appdata\\local\\pip\\cache\\wheels\\eb\\95\\6e\\3d5c21d6b340b7b64a86b7926293decf7f6991f67f914819ae\n",
      "Successfully built traits\n",
      "Installing collected packages: looseversion, traits, simplejson, pydot, nibabel, isodate, httplib2, configparser, configobj, ci-info, rdflib, pyxnat, etelemetry, prov, nipype, fitz\n",
      "Successfully installed ci-info-0.3.0 configobj-5.0.8 configparser-7.0.0 etelemetry-0.3.1 fitz-0.0.1.dev2 httplib2-0.22.0 isodate-0.6.1 looseversion-1.3.0 nibabel-5.2.1 nipype-1.8.6 prov-2.0.0 pydot-2.0.0 pyxnat-1.6.2 rdflib-7.0.0 simplejson-3.19.2 traits-6.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 20:20:33.263 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\harde\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "#sample input:\n",
    "#job description\n",
    "# At [Company X], we rely on insightful data to power our systems and solutions. We’re seeking an experienced data scientist to deliver insights on a daily basis. The ideal candidate will have mathematical and statistical expertise, along with natural curiosity and a creative mind. While mining, interpreting, and cleaning our data, this person will be relied on to ask questions, connect the dots, and uncover hidden opportunities for realizing the data’s full potential. As part of a team of specialists, the data scientist will “slice and dice” data using various methods and create new visions for the future.\n",
    "# skills\n",
    "# Seven or more years of experience in data science\n",
    "# Proficiency with data mining, mathematics, and statistical analysis\n",
    "# Advanced experience in pattern recognition and predictive modeling\n",
    "# Experience with Excel, PowerPoint, Tableau, SQL, and programming languages (ex: Java/Python, SAS)\n",
    "# Ability to work effectively in a dynamic, research-oriented group that has several concurrent projects\n",
    "job_Description =print(\"Enter Job Description\")\n",
    "skills = print(\"Enter Skills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model_name = 'bert-base-uncased'  # You can use 'roberta-base' or another model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embedding(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Average the embeddings\n",
    "    embedding = np.mean(outputs.last_hidden_state.cpu().numpy(), axis=1)\n",
    "    return embedding\n",
    "\n",
    "job_description_embedding = get_embedding(job_Description)\n",
    "skills_embedding = get_embedding(skills)\n",
    "\n",
    "# Function to find the most suited resume and list all resume indices with their cosine similarity\n",
    "def find_most_suited_resume(preprocessed_files):\n",
    "    highest_similarity = -1\n",
    "    most_suited_resume_index = -1\n",
    "    resume_similarities = []  \n",
    "\n",
    "    for idx, resume_text in enumerate(preprocessed_files):\n",
    "        resume_embedding = get_embedding(resume_text)\n",
    "        \n",
    "        similarity_description = cosine_similarity(job_description_embedding, resume_embedding)[0][0]\n",
    "        similarity_skills = cosine_similarity(skills_embedding, resume_embedding)[0][0]\n",
    "        \n",
    "        average_similarity = (similarity_description + similarity_skills) / 2\n",
    "        \n",
    "        resume_similarities.append((idx, average_similarity))\n",
    "        \n",
    "        if average_similarity > highest_similarity:\n",
    "            highest_similarity = average_similarity\n",
    "            most_suited_resume_index = idx\n",
    "    \n",
    "    return most_suited_resume_index, highest_similarity, resume_similarities\n",
    "\n",
    "most_suited_resume_index, highest_similarity, resume_similarities = find_most_suited_resume(preprocessed_files)\n",
    "\n",
    "print(f\"The most suited resume is at index {most_suited_resume_index} with cosine similarity {highest_similarity}\")\n",
    "\n",
    "print(\"All resume indices with their cosine similarity:\")\n",
    "for idx, similarity in resume_similarities:\n",
    "    print(f\"Resume index {idx}: cosine similarity {similarity}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
